---
title: 播放器技术一-架构设计
date: 2019-09-17 13:22:14
categories:
  - 直播
tags:
  - 直播
---

# 功能特性

归纳起来，播放器主要有如下 3 个方面的功能特性：

- 读取（IO）：“获取” 内容  -> 从 “本地” or “服务器” 上获取

- 解析（Parser & Demuxer）：“理解” 内容  -> 参考 “格式&协议” 来 “理解” 内容

- 渲染（Render）：“展示” 内容  -> 通过扬声器/屏幕来 “展示” 内容

# 数据流/功能模块

把3 个功能串起来，就构成了整个播放器的数据流，如图所示：

{% asset_img 播放器数据流.png %}

## IO

负责数据的读取。

从数据源读取数据有多种标准协议，比如常见的有：File，HTTP(s)，RTMP，RTSP 等

## Parser & Demuxer

负责数据的解析。

音视频数据的封装格式，都有着各种业界标准，只需要参考这些行业标准文档，即可解析各种封装格式，比如常见的格式：mp4，flv，m3u8，avi 等。

muxer是指合并文件，即将视频文件、音频文件和字幕文件合并为某一个视频格式。比如把rmvb格式的视频，mp3格式的音频文件以及srt格式的字幕文件，合并成为一个新的mp4或者mkv格式的文件。

demuxer是muxer的逆过程，就是把合成的文件中提取出不同的格式文件。

## Decoder

其实也属于数据解析的一种，只不过更多的是负责对压缩的音视频数据进行解码，拿到原始的 YUV 和 PCM 数据，常见的视频压缩格式如：H.264、MPEG4、VP8/VP9，音频压缩格式如 G.711、AAC、Speex 等

## Render

负责视频数据的绘制和渲染，是一个平台相关的特性，不同的平台有不同的渲染 API 和方法，比如：Windows 的 DDraw/DirectSound，Android 的 SurfaceView/AudioTrack，跨平台的如：OpenGL 和 ALSA 等

# 模块设计

下面我们逐一剖析一下播放器整个数据流的每一个模块的 **输入和输出** ，并一起设计一下每一个模块的 **接口 API** 。

## IO模块

{% asset_img IO模块.png %}

### 输入

数据源的地址（URL），这个 URL 可以是一个本地的文件路径，也可以是一个网络的流地址。

### 输出

二进制的数据，即通过 IO 协议读取的音视频二进制数据。

### API

播放器 IO 模块的接口设计如下所示：

{% asset_img IO模块API.png %}

#### 打开/关闭视频流

Open/Close

播放器内核可以通过 URL 的头（Schemes）知道需要采用哪一种 IO 协议来拉流（如：FILE/RTMP/HTTP），然后通过继承本接口的子类去完成实际的协议解析和数据读取。

#### 读取数据

Read 方法用于顺序读取数据

ReadAt 用于从指定的 Offset 偏移的位置读取数据，主要用于文件或者视频点播，为播放器提供 Seek 能力。

#### 断线重连

网络流，可能出现断线的情况，因此独立出一个 Reconnect 接口，用于提供重连的能力。

## Parser & Demuxer 模块

{% asset_img 解析模块.png %}

### 输入

由 IO 模块读取出来的 bytes 二进制数据

### 输出

音视频的媒体信息，未解码的音频数据包，未解码的视频数据包

#### 音视频的媒体信息

主要包括如下内容：

- 视频时长、码率、帧率等
- 音频的格式：编码算法，采样率，通道数等
- 视频的格式：编码算法，宽高，长宽比等

### API

{% asset_img 解析模块API.png %}

#### 创建/释放解析对象

Create/Release

#### 解析音视频的媒体信息

Parse 解析

Get方法 获取媒体信息（时长，比特率等）

#### 读取分离的音视频数据包

Read

## Decoder模块

{% asset_img 解码模块.png %}

### 输入

音视频的媒体信息，未解码的音频数据包，未解码的视频数据包

### 输出

解码的音频/图像的原始数据，即 PCM 和 YUV

### API

#### 生产消费者模式

由于音视频的解码，往往不是每送入×××一帧数据就一定能输出一帧数据，而是经常需要缓存几帧参考帧才能拿到输出，所以编码器的接口设计常常采用一种 “生产者-消费者” 模型，通过一个公共的 buffer 队列来串联 “生产者-消费者”，如下图所述（截取自 Android MediaCodec 编解码库的设计）：

{% asset_img 解码模块API-生产消费者模式.png %}

{% asset_img 解码模块API.png %}

#### 创建/关闭解码器

Open/Close

#### 配置解码器

Configure

#### 入队列/出队列

QueueBuffer 送数据进入解码缓冲区队列，

DequeueBuffer 从解码缓冲区队列取数据

## Render模块

{% asset_img 渲染模块.png %}

### 输入

解码的音频/图像的原始数据，即 PCM 和 YUV

### 输出

Vie 和 Loudspeaker

### API

不同平台的窗口绘制和扬声器播放的系统层 API 都不太一样，但是接口层面的流程都差不多，如图所示：

{% asset_img 渲染模块API-音频.png %}

{% asset_img 渲染模块API-视频.png %}

#### 初始化

Init/UnInit

#### 设置窗口

SetView

音频渲染不需要

#### 设置渲染参数

SetParam

#### 渲染

Render

# 多线程模型

## 单线程模型的缺陷

截止目前，播放器的数据流，是一个单线程的结构

会存在如下几个问题：

1. 音视频分离后 -> 解码 -> 播放，中间无法插入逻辑进行**音画同步**
2. 无数据缓冲区，一旦网络/解码抖动 -> 导致**频繁卡顿**
3. 单线程运行，没有充分利用 CPU 多核

## 多线程模型

可以以数据的 “生产者 - 消费者” 为边界，添加数据缓冲区，将单线程模型，改造为多线程模型（IO 线程、解码线程、渲染线程），如图所示：

{% asset_img 多线程模型.png %}

新增了两个缓冲区（区别于单线程模型解码器内部缓冲区）

{% asset_img 单线程多线程对比.png %}

## 优势

改造为多线程模型后，其优势如下：

1. 帧队列（Packet Queue）：可抵抗网络抖动
2. 显示队列（Frame Queue）：可抵抗解码/渲染的抖动
3. 渲染线程：添加 AV Sync 逻辑，可支持音画同步的处理
4. 并行工作，高效，充分利用多核 CPU


# SDK 接口设计

前面详细介绍了播放器内涵的关键架构设计和数据流

如果期望以该播放器内核作为 SDK 给 APP 提供底层能力的话，还需要设计一套易用的 API 接口

API 接口，其实可抽象为如下 5 大部分：

1. 创建/销毁播放器。Create/Release/Reset

2. 配置参数（如：窗口句柄、视频 URL、循环播放等）。SetDataSource/SetOptions/SetView/SetVolume

3. 发送命令（如：初始化，开始播放，暂停播放，拖动，停止等）。Prepare/Start/Pause/Stop/SeekTo

4. 音视频数据回调（如：解码后的音视频数据回调）。SetXXXListener/OnXXXCallback

5. 消息/状态消息回调（如：缓冲开始/结束、播放完成等）

# 状态模型

总体来说，播放器其实是一个状态机，被创建出来了以后，会根据应用层发送给它的命令以及自身产生的事件在各个状态之间切换，可以用如下这张图来展示：

{% asset_img 状态模型.png %}

播放器一共有 9 种状态，其中，Idle 是创建后/重置后的到达的初始状态，End 和 Error 分别是主动销毁播放器和发生错误后进入的最终状态（通过 reset 重置后可恢复 Idle 状态）

# 参考&扩展

- [播放器技术分享（1）：架构设计](https://blog.51cto.com/ticktick/2324928) 七牛-卢俊
